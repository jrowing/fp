<%
  require "./scripts/eruby_util.rb"
%>

<%
  chapter(
    '01',
    %q{Scalars and vectors},
    'ch:vectors'
  )
%>

<% begin_sec("Continuity in nature",nil,'continuity-in-nature') %>

Ancient philosophers like Heraclitus and Parmenides quarreled over whether change
was real or only an illusion. Heraclitus was the one who famously said that you can't
step in the same river twice. From a modern perspective, a more interesting question
is whether change can be discontinuous. Leibniz claimed, less famously,
that ``nature never jumps,'' and his assertion has held up surprisingly well through
multiple revolutions in physics. 

<% marg(30) %>
<%
  fig(
    'leibniz-portrait',
    %q{Gottfried Wilhelm Leibniz (1646-1716) was a philosopher and one of the co-inventors of the calculus.}
  )
%>
<% end_marg %>

One of these revolutions was the discovery of quantum
mechanics, and you've probably heard the phrase ``quantum leap.'' Quantum leaps are not
actually leaps at all. For example, when an atomic nucleus undergoes radioactive decay,
it doesn't leap from state A to state B. If we prepare it in state A, then it gradually
transforms itself into a mixture of A and B, with the strength of the A part decaying exponentially,
while B gets correspondingly stronger. This mixture can be interpreted in probabilistic terms,
with the probability of A going down while B's goes up. So although Leibniz died a couple of
centuries too early to have had any inkling of quantum mechanics, he ended up being undeservedly correct.

To make these ideas about continuity more rigorous, we start by making everything
into a set. This would have upset Leibniz, who was sincerely convinced that
``a point may not be considered a part of a line,'' but the point-set approach is how the modern mathematician
thinks about these things by default.\footnote{An alternative description, which would have
been more to Leibniz's liking, is given
by smooth infinitesimal analysis, described, e.g., in Bell, \emph{A primer of infinitesimal analysis}.}
In this approach, a set contains points, and a point may be, for example:
%
\begin{itemize}
\item a point in Euclidean space,
\item an event in spacetime, or
\item a state of a physical system such as a machine or a gas of helium atoms.
\end{itemize}
%
An idealized particle has zero size, so Euclidean space can be thought of as the set of all possible locations
of a particle. The prototypical example of an event in spacetime would be the collision of two particles,
which happens at a certain location in space and also at a certain time.

These sets aren't just grab-bags. They have additional structure. The most fundamental
type of structure they have is that of a \emph{manifold}.\index{manifold} Suppose we start
with a geometrical object such as the real number line, or the Euclidean plane, but
then systematically rob it of all structure relating to measurement, such as distances or angles.
Distances and angles would be \emph{too much} structure for our purposes in this very general context.
For example, there is no meaningful way of stating the angle between two different states
of a helium gas. So we tear out
the information about what's-how-far-from-what, but we retain what's-connected-to-what.
The result is a manifold, which we can think of as a rubber sheet that can be infinitely
stretched or compressed, but not cut or glued together.

As manifolds,
\begin{equation}
  \raisebox{-.13\height}{\includegraphics{ch01/figs/man-eq-small-circle}} = \raisebox{-.2\height}{\includegraphics{ch01/figs/man-eq-large-circle}},
\end{equation}
because without a system of measurement, we can't tell a small circle from a big one.
And without angular measurement, we can't tell that a square has corners,
\begin{equation}
  \raisebox{-.13\height}{\includegraphics{ch01/figs/man-eq-small-circle}} = \raisebox{-.13\height}{\includegraphics{ch01/figs/man-eq-square}}.
\end{equation}
These are one-dimensional manifolds, which we happen to have drawn on a two-dimensional page.
A theorem by Whitney says that it is always \emph{possible} to embed a manifold in some higher-dimensional
space in this way, but such an embedding is optional, not mandatory. If we imagine ourselves as
pointlike beings living in a universe consisting of a circle, then we would consider an hypothetical
second dimension to be nonexistent, because it would never show up in the result of any experiment.

Part of our notion of a manifold is a kind of democratic principle, that no point has special properties
that set it apart from any other point. For example, the figure eight curve
\raisebox{-.13\height}{\includegraphics{ch01/figs/man-figure-eight}} is not a manifold, because its point
of self-intersection is special. If we deleted it, the curve would be broken into two disconnected parts.

We can have manifolds with more than one dimension. For example, the surface of a coffee cup is the
same manifold as the surface of a doughnut,
\begin{equation}
  \raisebox{-.13\height}{\includegraphics{ch01/figs/man-eq-coffee-cup}} = \raisebox{-.13\height}{\includegraphics{ch01/figs/man-eq-doughnut}},
\end{equation}
because one can be transformed into the other by stretching. An $n$-dimensional manifold
is referred to as an $n$-manifold.

\begin{eg}{A robot camera}\label{eg:robot-camera}
Suppose we have a robot consisting of a car that runs along a track, with a rotating camera on a turret.
The state of the robot can be described by a combination of two pieces of data: its position on the track,
and the direction in which the camera points. Taken by themselves, these two parts of the robot's state
define two distinct 1-manifolds: a line and a circle. Taken together, they define a cylinder, which is
a 2-manifold.
\end{eg}

A connected manifold consists of a single piece. A disconnected manifold has multiple blobs that don't
touch. Because it seems that ``nature never jumps,'' physicists normally deal with connected manifolds.

Manifolds are discussed in more detail in section \ref{sec:manifolds}, p.~\pageref{sec:manifolds}.

<% end_sec('continuity-in-nature') %>

<% begin_sec("Vectors in too much generality",nil,'vectors-general') %>

Points in a manifold can be used to represent states, but they can
often also be used to represent changes in state. Examples include displacements along
a line, or rotations on a circle.
The order in which these motions occur may or may not matter.
The robot camera in example \ref{eg:robot-camera} can slide and then rotate, or rotate and then
slide, and the final result is the same state. If a ship sails 100 km north and then 100 km east, the
result is almost the same as going east and then north --- but not quite the same, because of the curvature
of the earth. We want to study the case in which the order doesn't matter, and the motions don't ``wrap
around'' as rotations do. This motivates the following definition.

\begin{important}[Definition of a vector space]
A vector space is a connected manifold that has defined on it an operation of addition
with the following properties:
\begin{enumerate}
\item Addition is commutative, $\vc{a}+\vc{b}=\vc{b}+\vc{a}$.
\item There is no nonzero $\vc{a}$ such that $\vc{a}+\vc{a}=\vc{0}$.
\end{enumerate}
\end{important}
\noindent Addition should also be associative, $(\vc{a}+\vc{b})+\vc{c}=\vc{a}+(\vc{b}+\vc{c})$,
and continuous, and have an identity element $\vc{0}$ and additive inverses.

<% marg() %>
<%
  fig(
    'parallelogram-rule',
    %q{The parallelogram rule used to define vector addition in example \ref{eg:parallelogram-rule}.}
  )
%>
<% end_marg %>

\begin{eg}{The parallelogram rule}\label{eg:parallelogram-rule}
The classic example that most people visualize when they think of a vector space is a space
made of arrows drawn in the euclidean plane. These arrows can be moved around without changing their
identity, as long as we don't rotate them. Addition is defined as shown in figure
\figref{parallelogram-rule}, and the figure makes the commutativity of addition
geometrically obvious. This type of vector, or its generalization to three dimensions, can be
used to describe physical quantities such as displacements (motion from one point to another),
forces, velocities, and magnetic fields.
\end{eg}

We distinguish vectors from variables representing ordinary numbers by
writing them either in bold face, $\vc{x}$, or with some kind of decoration,
called an ``index,'' above and to the right: $x^{\Arrow[.1cm]}$, $x^\star$,
$x^a$, $x^{\smiley}$.  
For compactness, we sometimes use an arrow and move it so
it lines up with the letter, $\vec{x}$. Although the arrow is common, for reasons that will become clear
later we will sometimes want to have more than one kind of decoration to use as an index.

Whenever we have a vector space, there is a natural notion of multiplying a vector by
a number, e.g., $2\vc{v}$ means $\vc{v}+\vc{v}$. By property 2 in the definition of a vector space,
$2\vc{v}$ is not zero unless
$\vc{v}=\vc{0}$.

\begin{eg}{Cutting a vector in half}
Given a vector $\vc{v}$, there is a uniquely defined half-size version $\vc{h}=\vc{v}/2$
defined by $\vc{h}+\vc{h}=\vc{v}$. For suppose
we have some other vector $\vc{i}$ such that $\vc{i}+\vc{i}=\vc{v}$.
Then we can write $(\vc{i}-\vc{h})+(\vc{i}-\vc{h})=\vc{0}$, so by property 2, $\vc{i}-\vc{h}=0$, and
$\vc{i}$ is the same as $\vc{h}$.
\end{eg}

Continuing in this way, we can define $(3/4)\vc{v}$ and so forth, so that multiplication
by any real number is well defined.\footnote{To handle irrational numbers, we can expand our number
as a nonterminating binary decimal, and we will need the completeness
property of manifolds defined on p.~\pageref{manifold-completeness} to ensure that the resulting
limit exists.}

\begin{eg}{Rotations are not vectors}
When we spin the hour on a clock, it doesn't matter whether we rotate it first by one hour and then by two,
or two first and then one, so commutativity is satisfied. But if we do a clockwise rotation by 6 hours
and then  a second such rotation, the hand is back where it started. This violates
property 2, so rotations don't qualify as vectors. For rotations in more dimensions, commutativity also
fails.
\end{eg}

<% end_sec('vectors-general') %>

<% begin_sec("Measuring vectors by addition",nil,'measuring-vectors-by-addition') %>

The definition of vectors given in section \ref{sec:vectors-general}
is a very general mathematician's definition, and it covers examples
like the vector space of polynomials or a vector space used in economics
to describe how a factory allocates its workers.
Physicists usually use the term to imply a more restrictive
definition, which is vector spaces that in a certain sense have the
same structure as either space (three dimensions) or spacetime (four).
We refer to these as 3-vectors and 4-vectors.

To define ``a certain sense,'' we have to consider what it means to
measure a vector. The qualitative idea is that vectors are
\emph{relational}: if we wish to use measurements in order to fully
characterize a nonzero vector, we can do so only in
relation to some measuring device. For example, consider a propeller anemometer, which is
a device used to measure the speed of the wind. We consider a simple version of the device in
which the axis is held fixed by the user during a particular measurement. If the
wind shifts from the north to the south, then the propeller will spin the opposite way about the fixed axis,
and we will say that the wind's velocity vector $\vc{v}$ has changed to $-\vc{v}$. But
exactly the same result is obtained if we use our hand to turn the anemometer around.
The instrument doesn't ``know'' whether it turned around or the wind did. Its measurement
only tells us the direction of the wind \emph{in relation to} its own orientation.

To crystallize this intuition, we make two seemingly weak assumptions about measurement:

\begin{enumerate}\label{measurement-assumptions}
\item We can measure whether a vector is zero.
\item We can physically create the sum of two vectors.
\end{enumerate}

\noindent The idea expressed by assumption 1 is that a zero vector is the only vector we can
measure in absolute terms, rather than in relation to another vector.
Note that, as we'll see in the next section, we cannot in general
determine whether a vector is zero by measuring whether its
\emph{magnitude} is zero. A 4-vector can be nonzero and yet have a nonzero magnitude.

\begin{eg}{Opposite, equal, and parallel vectors}\label{eg:measuring-vectors-opposite}
It follows from these assumptions that we can tell whether two vectors are opposite, $\vc{a}=-\vc{b}$.
If they are opposite, then we can physically create their sum (assumption 2) and measure that the result is zero
(assumption 1). 

For example, we will see later that forces are vectors, and it makes sense that we can tell when two forces are opposite.
I have a coffee cup sitting on my desk right now, and the downward force of the earth's gravity on the cup is
opposite to the upward force from the table that supports it.

By similar logic, we can measure whether two vectors are equal, $\vc{a}=\vc{b}$, and
whether they are parallel, with $\vc{a}=\alpha\vc{b}$ for some $\alpha$.
\end{eg}

\begin{eg}{Zero magnetic field}\label{eg:zero-magnetic-field}
We can use a magnetic compass with a frictionless bearing to measure
whether a magnetic field is zero. If the field is nonzero, then the needle can be made to oscillate around
its preferred orientation. If the field is zero, then such an oscillation is impossible, and the needle
can only spin at constant speed or stay put if released at rest in an arbitrary orientation.
\end{eg}

<%
 fig(
   'nulling-magnetic-fields',
   %q{Example \ref{eg:nulling-magnetic-fields}.
      Magnetic fields are produced by two solenoids: cylindrical coils of wire with electric currents
      running through them. The solenoids are shown in cross-section,
      with empty space on their interiors and their common axis running right-left. By symmetry, the field
      vectors at points  along this axis are parallel to the axis.},
   {
     'width'=>'wide',
     'sidecaption'=>true
   }
 )
%>

\begin{eg}{Strength of a magnetic field}\label{eg:nulling-magnetic-fields}
Figure \figref{nulling-magnetic-fields} shows
a method of measuring how a magnetic field falls off with distance.
Each of the two solenoids produces its own contribution to the field measured at the active tip of the sensor wand.
In such a situation, the fields add as required by assumption 2, and the sensor can tell whether
the field is zero at its own tip. This is similar to example \ref{eg:zero-magnetic-field}, but the
technique is extremely precise because the electronic probe can be used on a very sensitive setting.
If the fixed solenoid produces a magnetic field $\vc{B}$
when unit current flows through it, then increasing the current by a factor $f$ gives
a field $f\vc{B}$. By varying the position $r_m$ of the moving solenoid, and redetermining the value of $f$
that produces zero total field, we can find out by what factor the field has changed.
\end{eg}

<% marg(-10) %>
<%
  fig(
    'basis',
    %q{The vectors $\vc{u}$ and $\vc{v}$ form a basis for a plane. Any other vector can be expressed
       as a linear combination of them.}
  )
%>
<% end_marg %>

The ability to measure equality of vectors
implies that vectors have a sort of portable identity, as when you travel to a foreign country
but remain the same person. Similarly, our assumptions imply that we can test whether a vector
measured in San Francisco is the same as a some other vector measured in Nairobi. For example, a pencil
represents a vector. We could attach a pencil to a gyroscope, transport it around the world, and then compare
it to some other pencil, and verify whether they coincided in their lengths and directions. The use of the gyroscope
guarantees that while we transport the pencil, it will remain parallel to its original direction. This type
of operation is referred to as \emph{parallel transport}.\index{parallel transport} It turns out that parallel
transport is always possible in special relativity, but not in Einstein's theory of general relativity, which
says that gravitational fields are a kind of curvature of spacetime. The curvature messes up the parallel transport
in certain ways and makes it unreliable or ambiguous.

It is helpful to have the ideas of a linear combination
and a basis. If $\vc{u}$ and $\vc{v}$ are vectors, then for any real numbers $\alpha$ and $\beta$,
the expression $\alpha\vc{u}+\beta\vc{v}$ is called a linear combination of $\vc{u}$ and $\vc{v}$.
If we have a manifold of dimension $n$ that is a vector space, then it is always possible to find
a list of $n$ vectors such that every vector in the space can be expressed as a linear combination of
these vectors. Such a list is called a basis, and the coefficients $\alpha$ and $\beta$ are called
the vector's components in that basis. If this is the first time you've seen these concepts from linear
algebra, then you may want to look at a linear algebra textbook to gain a better \ldots basis in the
subject. I recommend \textbf{Linear Algebra} by Hefferon, which is free online.

<%
 fig(
   'change-of-basis',
   %q{The same vector is expressed in four different choices of basis.},
   {
     'width'=>'wide',
     'sidecaption'=>true,
     'sidepos'=>'b'
   }
 )
%>

A choice of basis is arbitrary. Figure \figref{change-of-basis} shows that when we change our basis,
we get different components. Mathematically, this is just a matter of algebra, e.g., $\vc{c}=-\vc{a}$ and
$\vc{d}=-\vc{b}$, so by substitution $\vc{a}+\vc{b}=-\vc{c}-\vc{d}$. But physically, our assumptions about
measurement imply that we can measure a vector's components, and therefore a change of basis must have
a physical interpretation. If the two dimensions in this example are both spatial rather than temporal,
and the vectors are magnetic field vectors, then the change of basis from $\langle \vc{a},\vc{b} \rangle$
to $\langle \vc{c},\vc{d} \rangle$ is a rotation of two magnetic field detectors by 180\degunit,
$\langle \vc{a},\vc{b} \rangle \rightarrow \langle \vc{e},\vc{f} \rangle$ means cutting the units
of measurement in half, and
$\langle \vc{a},\vc{b} \rangle \rightarrow \langle \vc{g},\vc{h} \rangle$ is accomplished by flipping (reflecting)
the configuration of the detectors.

A physicist thinks of a displacement vector and a magnetic field vector as inhabiting different vector
spaces, because it doesn't make sense to add a displacement to a magnetic field. However, all such
vector spaces are related to one another, because any change of basis in one of them can be related
to a change of basis in another. For example, if a laboratory on the deck of a ship contains magnetic field
sensors and wind sensors, then rotating the ship by 180\degunit effects the same change of basis for both types
of sensors. This is what a physicist means by a vector, and it is a more restrictive definition than
a mathematician's:

\begin{important}[Definition of vectors and scalars (in physics)]
A 3-vector is a vector that transforms under a change of basis in the same way as a spatial displacement.

\noindent A 4-vector is a vector that transforms under a change of basis in the same way as a spacetime displacement.

\noindent A scalar is a quantity that does not change at all under a change of basis.
\end{important}

<% marg(300) %>
<%
  fig(
    'vectors-and-scalars',
    %q{Temperature is a scalar. Force is a vector.}
  )
%>
<% end_marg %>

Temperature is a scalar, because a hot cup of coffee doesn't change its temperature
when we turn it around. Force is a vector. When I play tug-of-war with my dog,
her force and mine are the same in strength, but they're in opposite directions.
If we swap positions, our forces reverse their directions, just as an arrow representing a spatial
displacement would.

<% begin_sec("Motion and inertia",nil,'motion-and-inertia') %>

Consider
a vector representing a displacement in three-dimensional space, visualized abstractly
as an arrow. Assumption 1, that we can tell whether a vector is zero, seems obviously to be satisfied.
But now suppose I drive to Gettysburg, Pennsylvania, and visit the brass plaque marking the site
of the momentous Civil War battle. Am I really in the same spot? The earth has been spinning
and orbiting the sun since 1863.

Motion occurs in spacetime, which is a four-dimensional
manifold. There was an event A, the battle, and an event B, my arrival at the scene.
If A and B are the same point in this four-dimensional space, then I'm there on the day
of the battle, with bullets flying past my head, and it is indisputable that the displacement 4-vector is
zero. But in general we \emph{cannot} tell whether a 4-vector is zero just by examining its spatial part.
<% marg(-300) %>
<%
  fig(
    'gettysburg',
    %q{A different choice of basis in spacetime can mean a different definition of what is at rest.
       Two spatial dimensions are suppressed in this one-dimensional diagram, so the bases
       $\langle \vc{t},\vc{x} \rangle$ and $\langle \boldsymbol{\tau},\boldsymbol{\xi} \rangle$ have
       only two elements each rather than four.}
  )
%>
<% end_marg %>

When I make observations in four-dimensional spacetime, I also have to
choose a timelike basis vector, call it $\vc{t}$,
such that if the tail of the vector is at an event like the battle of Gettysburg,
its tip is at an event that I consider to occur at the same place but a later time.
This choice of $\vc{t}$ has an element of arbitrariness. An observer in a different state
of motion would make a different choice, as shown in figure \figref{gettysburg}. If we express
the earth's displacement vector in basis $\langle \vc{t},\vc{x} \rangle$, it has both a spatial
component and a temporal component, so we say the the earth moves. If we choose
$\langle \boldsymbol{\tau},\boldsymbol{\xi} \rangle$ instead, then we have effectively chosen
a frame of reference tied to the earth, and we say that the earth's displacement is purely temporal.

Because 3-vectors live in a three-dimensional vector space that can be
embedded within the four-dimensional spacetime, a mathematician would
consider every 3-vector to be a valid 4-vector. By the physicist's
definition, we reach the opposite conclusion. 

Suppose that an object traces out a track through spacetime such that
all displacements along that track are parallel to one another.
Then we say that its motion is \emph{inertial}. Inertial motion is
empirically verifiable, whereas being at rest is not. For example,
suppose that the glass of water in figure \figref{beer} is sitting on
a level table in a train's dining car. The surface of the water is
tilted. We can infer from this that the train's motion is noninertial.
But we \emph{cannot} tell anything about how fast it is moving, or
whether it is moving at all. We can guarantee that there exists some
frame of reference in which the train is at rest, as well as many others
according to which it is moving.
The beginning of modern physics was the realization by Galileo and others that
inertial motion is the natural state of motion of an object, ``natural'' meaning what it does
if it is not interacting with any other objects.
<% marg(50) %>
<%
  fig(
    'beer',
    %q{Evidence of noninertial motion.}
  )
%>
<% end_marg %>



An important example of a scalar is the number of ticks on a clock.
Since this is just a count, like the number of apples in a sack, its
value can't depend on our choice of basis. We call this count of the
ticks \emph{proper time}, where the word ``proper'' is used in the
sense of ``self'' or ``one's own,'' as in ``the Vatican doesn't lie
in Rome proper.''
This does not, however, mean that time in general is
absolute or independent of our frame of reference --- as we will see
in our study of Einstein's theory of relativity, time in general is not a scalar. Only
proper time is.

When we multiply (or divide) a vector by a scalar, we get another
vector (by either the mathematician's definition or the physicist's).
Therefore if we divide a displacement 3-vector by a time (or a proper
time, when the distinction is important), we get a valid 3-vector. In
this way, we can define a velocity vector. Under conditions in which the effects of
relativity are negligible, velocity vectors add in
relative motion (satisfying assumption 2 about measurement on
p.~\pageref{measurement-assumptions}).  If object A is moving relative
to object B with velocity $\vc{v}_\text{AB}$, and B relative to C at
$\vc{v}_\text{BC}$, then the velocity of A relative to C is given by
\begin{equation}
  \vc{v}_\text{AC} = \vc{v}_\text{AB}+\vc{v}_\text{BC} \qquad \text{[nonrelativistic approximation]}.
\end{equation}
<% end_sec('motion-and-inertia') %>


<% end_sec('measuring-vectors-by-addition') %>

<% begin_sec("Measuring vectors with the inner product",nil,'inner-product') %>
We now have a very limited notion of numerical measurement, which allows us to measure proper time intervals.
We would like to expand this to include other types of measurement such as distances and angles.
You began learning to do this in elementary school, but many of the ideas you soaked up about these
things turn out to be wrong when we consider relativistic effects. We would therefore like to develop
the topic using an elegant bare minimum of conceptual apparatus, in such a way that there are no
wrong assumptions that are swept under the rug.

<% marg(50) %>
<%
  fig(
    'time-triangle',
    %q{Clock 1 moves along $\vc{a}$, 2 along $\vc{b}$ and $\vc{c}$.}
  )
%>
<% end_marg %>

Suppose that clock 1 moves inertially through spacetime, tracing out vector $\vc{a}$. Clock 2 is
also present at the events that lie at the tail and tip of $\vc{a}$, but it moves noninertially,
tracing out vectors $\vc{b}$ and $\vc{c}$, so that $\vc{a}=\vc{b}+\vc{c}$. That is, clock 2 cruises smoothly away
from clock 1, but then abruptly accelerates and then cruises smoothly back to a reunion. There are two
logical possibilities:
\begin{enumerate}[label=(\roman*)]
\item The reunited clocks will always agree on the elapsed proper time.
\item The clocks will in general disagree.
\end{enumerate}
There is of course the degenerate case where $\vc{a}$, $\vc{b}$, and $\vc{c}$ are all parallel, so that the clocks are
never really separated, and 2's motion is also inertial. In this case, the triangle representing the vector
addition collapses to a one-dimensional figure, there is nothing to distinguish the
two clocks, and they must agree. Furthermore, we expect that since ``nature never jumps,'' the clocks must
approximately agree when we approximate the degenerate case. That is, the result of the experiment
must be a continuous function of the experimental parameters.
Thus if (ii) holds, we may be fooled by experiments in which the motion is slow into believing that
(i) is really the truth. This is in fact what happened historically, leading to the belief by Newton (which,
to his credit, he realized was a nontrivial assumption) that
``Absolute, true and mathematical time, of itself, and from its own nature flows equably without regard 
to anything external\ldots'' When we do experiments at high enough speeds or with sufficiently accurate
clocks, we find that (ii) is actually correct.

These considerations lead to a surprisingly simple, unified theory of measurement based on the assumption
that there is a kind of multiplication operation for four-vectors. We will use more than one way of notating
this operation, but for the moment we will use a dot, $\vc{a}\cdot\vc{b}$. It has the following properties:
\begin{itemize}
\item This product is a physical scalar, i.e., it has a value that is independent of the frame of reference or 
         choice of basis.
\item It is commutative, $\vc{a}\cdot\vc{b}=\vc{b}\cdot\vc{a}$.
\item It is linear, $\vc{a}\cdot(u\vc{b}+v\vc{c})=u\vc{a}\cdot\vc{b}+v\vc{a}\cdot\vc{c}$.
\end{itemize}
For convenience, we write $a^2=\vc{a}\cdot\vc{a}$, with the ``a'' in $a^2$ written without bold face
to indicate that it is a scalar.

We assume that this system is complete, in the sense that there is nothing else that we can
measure about vectors besides inner products. It then follows that when a clock moves along
a vector such as $\vc{a}$, the proper time $\tau$ is expressible in terms of $a^2$, and since the count
of the clock ticks is additive, we must have $\tau^2 \propto a^2$. The constant of proportionality
has an absolute value that depends on our choice of units, and may also have a sign. It is conventional
to take the constant to be either $+1$ or $-1$, and unfortunately there is no universally recognized convention
as to the sign. We will avoid picking such a convention explicitly until we come back to our study of
relativity in more detail.

It follows from these assumptions that the inner product of spacetime displacement vectors is measurable
using clocks. For example, let clocks 1 and 2 move as described
earlier, tracing out a triangle representing the sum
$\vc{a}=\vc{b}+\vc{c}$. Then
$c^2=(\vc{a}-\vc{b})^2=a^2+b^2-2\vc{a}\cdot\vc{b}$, and everything in
this equation is directly measurable using a clock, except for
$\vc{a}\cdot\vc{b}$, which can then be determined.\footnote{In this
example we assumed that it was possible to move clocks along the given
spacetime displacements, but this assumption can be relaxed. For
example, suppose that $\vc{b}$ points backward in time. Then $-\vc{b}$
points forward in time, and we have
$\vc{a}\cdot\vc{b}=-(\vc{a}\cdot-\vc{b})$.}

When $\vc{a}\cdot\vc{b}=0$, we say that $\vc{a}$ and $\vc{b}$ are orthogonal to one another.
If $\vc{a}$ and $\vc{b}$ represent spatial displacements, then this has the interpretation
that you would expect from your study of the Euclidean dot product: they are at a right
angle to one another. If, on the other hand, $\vc{u}$ is traced out by an observer (so that
it points into the future), then $\vc{u}\cdot\vc{b}=0$ clearly cannot be interpreted as
describing a right angle. Instead, we interpret this as meaning that events at the tip
and tail of $\vc{b}$ (when $\vc{b}$ is placed in some position) are simultaneous as judged
by that observer. Another observer, with $\vc{v}\ne\vc{u}$, will find $\vc{v}\cdot\vc{b}\ne 0$,
and will say that the events are \emph{not} simultaneous. 

It is not our purpose at this point to develop the theory of special relativity in detail,
but the following explicitly relativistic experiment will help in clarifying a couple of points.
Figure \figref{maximal-proper-time} is a variation on figure \figref{time-triangle}, with the
vector $\boldsymbol{\delta}$ measuring the deviation of the second clock from inertial motion.
Let $\boldsymbol{\delta}=r\vc{x}$, where $r$ is an adjustable parameter and $\vc{x}$ is a fixed vector
orthogonal to $\vc{i}$ (i.e., the inertial observer says that $\vc{x}$ is a vector connecting simultaneous
points).
Experiments verify hypothesis (ii) above, that the proper time $\tau$ along the noninertial path varies
with $r$. By symmetry, $\tau$ must be an even function of $r$, so we expect that it will be at an
extremum for $r=0$. Experiments show that it is a maximum. Such observations lead to the
\emph{principle of maximal proper time},
that proper time is maximized by inertial motion.
<% marg(50) %>
<%
  fig(
    'maximal-proper-time',
    %q{Proper time is maximized when $\delta=0$.}
  )
%>
<% end_marg %>

The proper time along vector $\vc{i}+\boldsymbol{\delta}$ is
$\sqrt{|(\vc{i}+\boldsymbol{\delta})^2|}$, where the absolute value symbols are inserted so that
our equations will be true regardless of one's decision about the arbitrary sign convention
alluded to above. Multiplying out the square and making use of
$\vc{i}\cdot\boldsymbol{\delta}=0$, we find that the proper time is $\sqrt{|i^2+\delta^2|}$. 
(Recall that a notation like $\delta^2$ is just an abbreviation for $\boldsymbol{\delta}\cdot\boldsymbol{\delta}$.
We have not defined any number called $\delta$.) The proper
time along $\vc{i}-\boldsymbol{\delta}$ is the same, so the total proper time
is $2\sqrt{|i^2+\delta^2|}$. By the principle of maximal time, this must be \emph{less}
than the proper time $2i$ of the inertial clock, and therefore $\vc{i}\cdot\vc{i}$ and
$\boldsymbol{\delta}\cdot\boldsymbol{\delta}$ apparently have opposite signs. This
is contrary to our Euclidean intuition, according to which the dot product of a vector
with itself is the square of its length, and lengths are always positive. But in the context
of spacetime, it actually makes some sense. The inner product is supposed to suffice as
a complete system of measurement in spacetime. If so, then there ought to be some way
to use inner products to distinguish between time and space. We have found that there is:
``timelike'' and ``spacelike'' displacements differ in the sign of their inner products with themselves.
If we had instead insisted that all vectors have positive inner products with themselves, then there
would have been nothing to distinguish time from space. We would be describing a strange universe
consisting of a four-dimensional Euclidean space, a universe in which an observer could turn 180 degrees and
rotate the future into the past.

This example also helps to explain the sense in which the physics of
Galileo and Newton (Galilean relativity) can be a valid low-velocity
approximation to the more generally correct picture provided by
Einstein's special relativity. If the Galilean approximation is good when
the motion of one object relative to another is slow, the natural question to
ask is ``slow compared to what?'' The proper time of the noninertial clock in our
example is $2\sqrt{|i^2+\delta^2|}$, and we can make this vanish by scaling $\boldsymbol{\delta}$ up to an
appropriately chosen size. This special size of $\boldsymbol{\delta}$ is the one for which
the number of units of time expressed by $\sqrt{|i^2|}$ is the same as the number of
units of space measured by $\sqrt{|\delta^2|}$. If time and space units are to be comparable
in this way, then there is some special speed, which is one space unit per time unit.
This speed is commonly referred to as the speed of light, $c$, but from a modern point of
view, it is more like a conversion factor between spatial and temporal units.
When we return later to a more detailed investigation of special relativity, we will
see that $c$ is a velocity that is agreed upon by all observers, and that it is not
actually possible to accelerate a clock or other material object to $c$.

<% end_sec('inner-product') %>


<% begin_hw_sec %>
<% begin_hw('measure-parallelism') %>
Section \ref{sec:measuring-vectors-by-addition}, p.~\pageref{sec:measuring-vectors-by-addition}, states
two assumptions about measurement. Based on these assumptions, can we measure
whether or not two vectors are parallel? Perpendicular?
<% end_hw %>

<% end_hw_sec %>

<% end_chapter %>
